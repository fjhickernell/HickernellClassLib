# Confidence Intervals

If

- $\theta$ is a parameter of interest of a distribution, and

- $X_1, \ldots, X_n$ are data that we assume are collected from that distribution,

then we try to construct random quantities $\Theta_L$ and/or $\Theta_U$,
depending only on the data (and not on $\theta$), that give intervals which
[capture]{.alert} $\theta$ with high probability $1-\alpha$.  Depending on the situation, this means constructing

- a two-sided interval with  $\Prob(\Theta_L \le \theta \le \Theta_U) \ge 1-\alpha$, or

- a one-sided lower interval with  $\Prob(\Theta_L \le \theta) \ge 1-\alpha$, or

- a one-sided upper interval with  $\Prob(\theta \le \Theta_U) \ge 1-\alpha$

The bounds $\Theta_L$ and $\Theta_U$ are random because they depend on random data. Here $\alpha$ is our willingness to be wrong, typically $\alpha = 5\%$.

- In many continuous cases, the probability is exactly $1-\alpha$
- For discrete distributions, the probability is often slightly larger than $1-\alpha$

---

- [Large sample size confidence intervals for means](#large-sample-confidence-intervals-for-means)
- [Small sample size confidence intervals for means when the distribution is known](#small-sample-confidence-intervals-for-means-when-the-distribution-is-known)
- [Confidence intervals for means of differences](#confidence-intervals-for-means-of-differences)
- [Confidence intervals for variances](#confidence-intervals-for-variances)

## Large sample size confidence intervals for means{#large-sample-confidence-intervals-for-means}   
If $X_1, \ldots, X_n$ are IID with mean $\mu$ and variance $\sigma^2 < \infty$, and $\barX_n$ is the sample mean, then by the [Central Limit Theorem](../slides/01-intro.html#clt)
$$
\frac{\barX_n - \mu}{\sigma/\sqrt{n}} \appxsim \Norm(0,1) \quad \text{for large } n
$$
Letting $z_{\alpha/2}$ be the upper $\alpha/2$ quantile of $\Norm(0,1)$, i.e., $z_{\alpha/2} = Q_{\Norm(0,1)}(1 - \alpha/2)$, then
\begin{align*}
1 - \alpha & \approx
\Prob \biggl( -z_{\alpha/2} \le \frac{\barX_n - \mu}{\sigma/\sqrt{n}} \le z_{\alpha/2} \biggr) \\
& \approx \Prob \biggl( \barX_n - z_{\alpha/2} \frac{\sigma}{\sqrt{n}} \le \mu \le \barX_n + z_{\alpha/2} \frac{\sigma}{\sqrt{n}} \biggr) \\
& \approx \Prob \biggl( \underbrace{\barX_n - z_{\alpha/2} \frac{S}{\sqrt{n}}}_{\Theta_L} \le \mu \le \underbrace{\barX_n + z_{\alpha/2} \frac{S}{\sqrt{n}}}_{\Theta_U} \biggr)
\end{align*}
where $S^2$ is some estimate of the unknown population variance $\sigma^2$ (e.g., unbiased or MLE)

---

See the [Approval Ratings](../slides/01-intro.html#approval-ratings) example for an illustration of this construction for a Bernoulli mean

```{python}

#| echo: false
#| output: asis

import math
import scipy.stats as st

# =========================
# Parameters (edit HERE)
# =========================
n_small = 16
n_large = 400

xbar = 12.0
alpha = 0.05

# =========================
# CI helpers
# =========================

def ci_exp_exact_mu(n, xbar, alpha=0.05):
    """Exact CI for Exp mean via chi-square."""
    df = 2*n
    q_lo = st.chi2.ppf(alpha/2, df)
    q_hi = st.chi2.ppf(1 - alpha/2, df)
    return (2*n*xbar)/q_hi, (2*n*xbar)/q_lo

def ci_clt_exp_mu(n, xbar, alpha=0.05):
    """
    CLT CI for Exp mean using plug-in sigma_hat = xbar.
    """
    z = st.norm.ppf(1 - alpha/2)
    se = xbar / math.sqrt(n)
    return xbar - z*se, xbar + z*se

def fmt_ci(lo, hi, digits=2):
    return f"$[{lo:.{digits}f},\\,{hi:.{digits}f}]$"

print(
    f"_Example_: You observe $\\barX_n = {xbar}$ minutes for taxis to arrive."
    f"You construct a ${100*(1-alpha):.0f}\\%$ confidence interval for the mean arrival time, $\\mu$, assuming that the arrival times are distributed  $\\Exp(1/\\mu)$.  Recall that $\\mu = \\sigma = 1/\\lambda$.\n"
)

print("| $n$ | *CLT* CI for $\\mu$ ($S=\\barX_{n}$) | CI width |")
print("|---:|:---:|---:|")
for n in (n_small, n_large):
    lo, hi = ci_clt_exp_mu(n, xbar, alpha)
    width = hi - lo
    print(f"| {n} | {fmt_ci(lo, hi)} | {width:.3f} |")

```

---

## Small sample size confidence intervals for means when the distribution is known{#small-sample-confidence-intervals-for-means-when-the-distribution-is-known}

If the sample size is not large enough for the [Central Limit Theorem](../slides/01-intro.html#clt) to apply, but the sample mean has a known distribution, then exact confidence intervals can sometimes be constructed

```{python}
#| echo: false
#| output: asis

print(
    f"_Example_: You observe $\\barX_n = {xbar}$ minutes for taxis to arrive."
    f"based on $n$. You construct a ${100*(1-alpha):.0f}\\%$ confidence interval for the mean arrival time, $\\mu$, assuming that the arrival times are distributed  $\\Exp(1/\\mu)$.  Recall that $\\mu = \\sigma = 1/\\lambda$. \n \n But now we have the true distribution of $\\barX_n$:"
)
```

\begin{align*}
2 \lambda n \barX_n \sim \chi^2_{2n} \implies 1 - \alpha &= \Prob \biggl( \chi^2_{2n, \alpha/2} \le 2 \lambda n \barX_n \le \chi^2_{2n, 1-\alpha/2} \biggr) \\
& = \Prob \biggl( \frac{\chi^2_{2n, \alpha/2}}{2 n \barX_n} \le \lambda \le \frac{\chi^2_{2n, 1-\alpha/2}}{2 n \barX_n} \biggr) \\
& = \Prob \biggl( \frac{2 n \barX_n}{\chi^2_{2n, 1-\alpha/2}} \le \mu \le \frac{2 n \barX_n}{\chi^2_{2n, \alpha/2}} \biggr)
\end{align*}

---
```{python}
#| echo: false
#| output: asis

print("| $n$ | *Exact* $\\chi^2$ CI | *Exact* CI width | *CLT* CI | *CLT* CI width | (CLT width/<br>Exact width) |")
print("|-:|:---:|----:|:---:|----:|-----:|")

for n in (n_small, n_large):
    clt_lo, clt_hi = ci_clt_exp_mu(n, xbar, alpha)
    ex_lo, ex_hi   = ci_exp_exact_mu(n, xbar, alpha)

    w_clt = clt_hi - clt_lo
    w_ex  = ex_hi - ex_lo

    print(
        f"| {n} | {fmt_ci(ex_lo, ex_hi)} | {w_ex:.3f} | "
        f"{fmt_ci(clt_lo, clt_hi)} | {w_clt:.3f} | {w_clt/w_ex:.3f} |"
    )

```

- For small $n$
  - Exact confidence interval can be substantially different from the CLT-based interval
  - CLT is symmetric about $\barX_n$, while the exact interval is not
- As $n$ increases, CLT-based interval approaches the exact interval

&nbsp;

<a class="button" style="margin-left:0em;" href="../classlib/classlib/notebooks/coverage_exact_vs_clt.ipynb" download>
â¬‡ Exact vs CLT Confidence Interval Coverage ðŸ““
</a>

&nbsp;

::: {.exitem}
<span class="exbullet">$\exstar$</span><span> You draw $n$ IID samples of your product to test for failure.  What is your confidence interval for $p$, the probability of a satisfactory product, if none of the samples fail?</span>
:::


## Studentâ€™s $t$ Confidence Interval for *Normal* Data with Unknown Variance

For $X_1,\dots,X_n \IIDsim \Norm(\mu, \sigma^2)$

- Sample mean: $\barX$
- Sample standard deviation: $S$

We have an [exact]{.alert} confidence interval for all $n$:
$$
\Prob\left[ \barX - t_{1-\alpha/2,n-1}\frac{S}{\sqrt{n}} \le \mu \le \barX + t_{1-\alpha/2,n-1}\frac{S}{\sqrt{n}} \right] = 1 - \alpha
$$
where $t_{1-\alpha/2,n-1}$ is the upper $\alpha/2$ quantile of the [Student's t distribution]{.alert} with $n-1$ degrees of freedom

- For large $n$, this interval is close to the CLT-based interval


## Confidence Intervals for _Means_ of Differences

For paired or matched data (before/after, twins, same subject measured twice)
$$
D_i = X_i - Y_i, \quad i = 1,\dots,n
$$

Inference is about the **mean difference** $\mu_D$

_Not_ difference of means $\mu_X - \mu_Y$, even though $\barD = \barX - \barY$

If $D_1,\dots,D_n \IIDsim \Norm(\mu_D, \sigma_D^2)$
$$
\Prob\left[ \barD - t_{1-\alpha/2,n-1}\frac{S_D}{\sqrt{n}} \le \mu_D \le \barD + t_{1-\alpha/2,n-1}\frac{S_D}{\sqrt{n}} \right] = 1 - \alpha
$$

If $D_1,\dots,D_n$ are IID with finite variance, and $n$ is large
$$
\Prob\left[ \barD - z_{1-\alpha/2}\frac{S_D}{\sqrt{n}} \le \mu_D \le \barD + z_{1-\alpha/2}\frac{S_D}{\sqrt{n}} \right] \approx 1 - \alpha
$$


## Confidence Intervals for _Differences_ of Means

**Typical setting:** two independent samples (control/treatment, two groups)
 
$$
X_1,\dots,X_{n_1} \sim \text{population 1}, \quad
Y_1,\dots,Y_{n_2} \sim \text{population 2}
$$

Parameter of interest: $\mu_1 - \mu_2$


**Two-sample *t* interval** (Wackerly, Variations exist, e.g., pooled *t*, Welch)

- Independent samples
- Normal populations
- Uses unpooled standard error
- Approximate distribution

**CLT-based interval**

- Assumes finite variances only
- Requires large $n_1, n_2$
- Often used when normality is doubtful

**Important:** independence between samples is essential.

## Confidence Intervals for _Proportions_

**Typical setting:** Bernoulli trials

Let  
\[
X \sim \text{Binomial}(n,p), \quad \hat p = X/n
\]

Parameter of interest: population proportion $p$.

**Common methods**

- **Normal approximation interval**
  - Requires $np$ and $n(1-p)$ large
  - Simple but can undercover

- **Wilson / Agrestiâ€“Coull interval**
  - Improved accuracy over normal approximation
  - Near-nominal coverage for moderate $n$

- **Exact (Clopperâ€“Pearson) interval**
  - No sample size restriction
  - Guaranteed coverage
  - Conservative (wider than necessary)

**Key tradeoff:** simplicity vs coverage accuracy.

## Confidence Intervals for _Variances_

**Typical setting:** inference about variability

Parameter of interest: population variance $\sigma^2$.

**Common methods**

- **Chi-squared interval**
  - Assumes data are IID Normal
  - Exact for any $n$
  - Very sensitive to non-normality

- **Asymptotic (CLT-based) intervals**
  - Assume finite fourth moments
  - Require large $n$
  - Rarely used in introductory courses

**Important:** variance inference is far less robust than mean inference.

## Assumptions Behind Common Confidence Intervals

Data are IID from a distribution with finite variance

| Parameter | Distributional Assumptions | Sample Size | Method | Notes |
|-------|--------------------------------|--------|-----------------------|----------------------------|
| $\mu$ | **Any** distribution | **Large** $n$ | Central Limit Theorem (CLT) | **Approximate**, accuracy improves as $n \to \infty$ |
| $\mu$ |  **Normal** data, <br>$\sigma$ unknown | Any $n$ | Studentâ€™s *t* | **Exact**|
| $\mu = p$ | **Bernoulli** trials | Any $n$ | Binomial<br>(Clopperâ€“Pearson) | **Exact**<br>Conservative |
| $\mu = p$ | **Bernoulli** trials | Large $np$,  $n(1-p)$ | Central Limit Theorem (CLT) | **Approximate** |
| $\mu$ | **Exponential** data | Any $n$ | Gamma/<br>Chi-squared  | **Exact** |

---

| Parameter | Distributional Assumptions | Sample Size | Method | Notes |
|-------|--------------------------------|--------|-----------------------|----------------------------|
| $\mu_D$ <br>(paired differences)| Differences are **Normal** | Any $n$ | Paired *t* |  **Exact**, sometimes confused with two-sample *t* |
| $\mu_1-\mu_2$ | Each sample **Normal**; independent samples | Any $n_1,n_2$ | Two-sample *t* (Wackerly) | **Approximate**, uses unpooled standard error |
| $\mu_1-\mu_2$  | Independent samples from **any** distributions with finite variances | **Large** $n_1,n_2$ | CLT (two-sample) | **Approximate**|
| 
| $\sigma^2$ | **Normal** data | Any $n$ | Chi-squared | **Exact**, sensitive to non-normality |
| $\med(X)$ | Continuous distribution | $n$ not too small | Order-statistics |**Approximate**, Distribution-free |  