# Confidence Intervals

If

- $\theta$ is a parameter of interest of a distribution, and

- $X_1, \ldots, X_n$ are data that we assume are collected from that distribution,

then we try to construct random quantities $\Theta_L$ and/or $\Theta_U$,
depending only on the data (and not on $\theta$), that give intervals which
[capture]{.alert} $\theta$ with high probability $1-\alpha$.  Depending on the situation, this means constructing

- a two-sided interval with  $\Prob(\Theta_L \le \theta \le \Theta_U) \ge 1-\alpha$, or

- a one-sided lower interval with  $\Prob(\Theta_L \le \theta) \ge 1-\alpha$, or

- a one-sided upper interval with  $\Prob(\theta \le \Theta_U) \ge 1-\alpha$

The bounds $\Theta_L$ and $\Theta_U$ are random because they depend on random data. Here $\alpha$ is our willingness to be wrong, typically $\alpha = 5\%$.

- In many continuous cases, the probability is exactly $1-\alpha$
- For discrete distributions, the probability is often slightly larger than $1-\alpha$

---

- [Critical value notation](#critical-value-notation-important)
- [Large sample size confidence intervals for means](#large-sample-confidence-intervals-for-means)
- [Small sample size confidence intervals for means when the distribution is known](#small-sample-confidence-intervals-for-means-when-the-distribution-is-known)
- [Confidence intervals for means of differences](#confidence-intervals-for-means-of-differences)
- [Confidence intervals for variances](#confidence-intervals-for-variances)


## Upper critical values{#critical-value-notation-important}

For a distribution with CDF $F$ and quantile function $Q$, define the upper critical value
$$
c_{\alpha} := Q(1-\alpha),  \quad \text{i.e., } F(c_{\alpha}) \ge 1-\alpha \text{ and } F(c_{\alpha} - \epsilon) < 1-\alpha \; \forall \epsilon > 0
$$

:::: {.columns}
::: {.column width="60%"}

Examples 

- $z_{\alpha} = Q_{\Norm(0,1)}(1-\alpha)$  
- $t_{\nu,\alpha} = Q_{t_\nu}(1-\alpha)$  
- $\chi^2_{\nu,\alpha} = Q_{\chi^2_\nu}(1-\alpha)$  

These [upper critical values]{.alert} are *not* $\alpha$-quantiles.

:::

::: {.column width="40%"}

```{python}
#| echo: false
#| fig-width: 4.8
#| fig-height: 3.6

import numpy as np
import matplotlib.pyplot as plt
import scipy.stats as st
plt.rcParams["mathtext.fontset"] = "cm"

alpha = 0.025
z = st.norm.ppf(1 - alpha)

x = np.linspace(-4, 4, 800)
y = st.norm.pdf(x)
y_z = st.norm.pdf(z)

fig, ax = plt.subplots()

# Density
ax.plot(x, y, linewidth=4)

# Critical value line: ONLY up to the pdf
ax.vlines(z, 0, y_z, linestyle="-", linewidth=4, color="tab:orange")

# Upper tail shading
xtail = x[x >= z]
ytail = st.norm.pdf(xtail)
ax.fill_between(xtail, 0, ytail, alpha=0.3, color="tab:orange")

# Title
ax.set_title(rf"Upper tail area $\alpha = {alpha} = {100*alpha}\%$", fontsize=24)

# Clean y-axis
ax.set_yticks([])

# Emphasize x-axis ticks
ax.tick_params(axis="x", labelsize=18)

# Mark z_alpha under the x-axis
ax.annotate(
    r"$z_{0.025}$",
    xy=(z, 0),
    xycoords=("data", "axes fraction"),
    xytext=(0, -18),
    textcoords="offset points",
    ha="center",
    va="top",
    fontsize=24
)

# Small tick at z_alpha
ax.plot([z, z], [0, -0.1],  linewidth=3, transform=ax.get_xaxis_transform(), color="tab:orange", 
        clip_on=False, solid_capstyle="butt")

plt.tight_layout()
plt.show()
```

:::

::::


## Large sample size confidence intervals for means{#large-sample-confidence-intervals-for-means}   
If $X_1, \ldots, X_n$ are IID with mean $\mu$ and variance $\sigma^2 < \infty$, and $\barX_n$ is the sample mean, then by the [Central Limit Theorem](../slides/01-intro.html#clt)
$$
\frac{\barX_n - \mu}{\sigma/\sqrt{n}} \appxsim \Norm(0,1) \quad \text{for large } n
$$
Letting $z_{\alpha/2}$ be the upper $\alpha/2$ quantile of $\Norm(0,1)$, i.e., $z_{\alpha/2} = Q_{\Norm(0,1)}(1 - \alpha/2)$, then
\begin{align*}
1 - \alpha & \approx
\Prob \biggl( -z_{\alpha/2} \le \frac{\barX_n - \mu}{\sigma/\sqrt{n}} \le z_{\alpha/2} \biggr) \\
& \approx \Prob \biggl( \barX_n - z_{\alpha/2} \frac{\sigma}{\sqrt{n}} \le \mu \le \barX_n + z_{\alpha/2} \frac{\sigma}{\sqrt{n}} \biggr) \\
& \approx \Prob \biggl( \underbrace{\barX_n - z_{\alpha/2} \frac{S_n}{\sqrt{n}}}_{\Theta_L} \le \mu \le \underbrace{\barX_n + z_{\alpha/2} \frac{S_n}{\sqrt{n}}}_{\Theta_U} \biggr)
\end{align*}
where $S_n^2$ is some estimate of the unknown population variance $\sigma^2$ (e.g., unbiased or MLE)

---

See the [Approval Ratings](../slides/01-intro.html#approval-ratings) example for an illustration of this construction for a Bernoulli mean

```{python}

#| echo: false
#| output: asis

import math
import scipy.stats as st

# =========================
# Parameters (edit HERE)
# =========================
n_small = 16
n_large = 400

xbar = 12.0
alpha = 0.05

# =========================
# CI helpers
# =========================

def ci_exp_exact_mu(n, xbar, alpha=0.05):
    """Exact CI for Exp mean via chi-square."""
    df = 2*n
    q_lo = st.chi2.ppf(alpha/2, df)
    q_hi = st.chi2.ppf(1 - alpha/2, df)
    return (2*n*xbar)/q_hi, (2*n*xbar)/q_lo

def ci_clt_exp_mu(n, xbar, alpha=0.05):
    """
    CLT CI for Exp mean using plug-in sigma_hat = xbar.
    """
    z = st.norm.ppf(1 - alpha/2)
    se = xbar / math.sqrt(n)
    return xbar - z*se, xbar + z*se

def fmt_ci(lo, hi, digits=2):
    return f"$[{lo:.{digits}f},\\,{hi:.{digits}f}]$"

print(
    f"_Example_: You observe $\\barX_n = {xbar}$ minutes for taxis to arrive."
    f"You construct a ${100*(1-alpha):.0f}\\%$ confidence interval for the mean arrival time, $\\mu$, assuming that the arrival times are distributed  $\\Exp(1/\\mu)$.  Recall that $\\mu = \\sigma = 1/\\lambda$.\n"
)

print("| $n$ | *CLT* CI for $\\mu$ ($S=\\barX_{n}$) | CI width |")
print("|---:|:---:|---:|")
for n in (n_small, n_large):
    lo, hi = ci_clt_exp_mu(n, xbar, alpha)
    width = hi - lo
    print(f"| {n} | {fmt_ci(lo, hi)} | {width:.3f} |")

```

---

## Small sample size confidence intervals for means when the distribution is known{#small-sample-confidence-intervals-for-means-when-the-distribution-is-known}

If the sample size is not large enough for the [Central Limit Theorem](../slides/01-intro.html#clt) to apply, but the sample mean has a known distribution, then exact confidence intervals can sometimes be constructed

```{python}
#| echo: false
#| output: asis

print(
    f"_Example_: You observe $\\barX_n = {xbar}$ minutes for taxis to arrive."
    f"based on $n$ observations. You construct a ${100*(1-alpha):.0f}\\%$ confidence interval for the mean arrival time, $\\mu$, assuming that the arrival times are distributed  $\\Exp(1/\\mu)$.  Recall that $\\mu = \\sigma = 1/\\lambda$. \n \n But now we have the true distribution of $\\barX_n$:"
)
```

::: {.columns}

::: {.column width="60%"}

\begin{align*}
2 \lambda n \barX_n &\sim \chi^2_{2n}  \\
\implies 1 - \alpha
&= \Prob \biggl( \chi^2_{2n, \alpha/2} \le 2 \lambda n \barX_n \le \chi^2_{2n, 1-\alpha/2} \biggr) \\
&= \Prob \biggl( \frac{\chi^2_{2n, \alpha/2}}{2 n \barX_n} \le \lambda \le \frac{\chi^2_{2n, 1-\alpha/2}}{2 n \barX_n} \biggr) \\
&= \Prob \biggl( \frac{2 n \barX_n}{\chi^2_{2n, 1-\alpha/2}} \le \mu \le \frac{2 n \barX_n}{\chi^2_{2n, \alpha/2}} \biggr).
\end{align*}

:::

::: {.column width="40%"}

```{python}
#| echo: false
#| fig-width: 4.8
#| fig-height: 3.4

import numpy as np
import matplotlib.pyplot as plt
import scipy.stats as st
plt.rcParams["mathtext.fontset"] = "cm"

n_plot = n_small
df = 2*n_plot
a2 = alpha/2

q_lo = st.chi2.ppf(a2, df)
q_hi = st.chi2.ppf(1 - a2, df)

c = 2*n_plot*xbar
mu_lo = c / q_hi
mu_hi = c / q_lo

# Change of variables: if Y ~ chi2_df and M = c/Y,
# then f_M(m) = f_Y(c/m) * (c/m^2)
def f_mu(mu):
    y = c/mu
    return st.chi2.pdf(y, df) * (c/(mu**2))

mu = np.linspace(0.6*mu_lo, 1.4*mu_hi, 900)
pdf = f_mu(mu)

fig, ax = plt.subplots()

# Density on mu-scale
ax.plot(mu, pdf, linewidth=4)

# Shade the CI region (blue)
mu_mid = mu[(mu >= mu_lo) & (mu <= mu_hi)]
ax.fill_between(mu_mid, 0, f_mu(mu_mid), alpha=0.3, color="tab:blue")

# Solid vertical lines at CI endpoints (blue) up to the curve
ax.vlines([mu_lo, mu_hi],
          0,
          [f_mu(mu_lo), f_mu(mu_hi)],
          linewidth=4,
          color="tab:blue")

# Label xbar under the axis
ax.annotate(
    rf"$\overline{{X}} = {xbar}$",
    xy=(xbar, 0),
    xycoords=("data", "axes fraction"),
    xytext=(0, -20),
    textcoords="offset points",
    ha="center",
    va="top",
    fontsize=18
)

ax.plot([xbar, xbar],
        [0, -0.05],
        transform=ax.get_xaxis_transform(),
        color="black",
        linewidth=2,
        clip_on=False)

ax.set_title(rf"Exact CI for $\mu$ via $\chi^2_{{{df}}}$ (central {100*(1-alpha):.0f}\%)", fontsize=24)
ax.set_yticks([])
ax.tick_params(axis="x", labelsize=18)

plt.tight_layout()
plt.show()
```

:::

:::

---
```{python}
#| echo: false
#| output: asis

print("| $n$ | *Exact* $\\chi^2$ CI | *Exact* CI width | *CLT* CI | *CLT* CI width | (CLT width/<br>Exact width) |")
print("|-:|:---:|----:|:---:|----:|-----:|")

for n in (n_small, n_large):
    clt_lo, clt_hi = ci_clt_exp_mu(n, xbar, alpha)
    ex_lo, ex_hi   = ci_exp_exact_mu(n, xbar, alpha)

    w_clt = clt_hi - clt_lo
    w_ex  = ex_hi - ex_lo

    print(
        f"| {n} | {fmt_ci(ex_lo, ex_hi)} | {w_ex:.3f} | "
        f"{fmt_ci(clt_lo, clt_hi)} | {w_clt:.3f} | {w_clt/w_ex:.3f} |"
    )
```
::: {.columns}

::: {.column width="45%"}

- For small $n$
  - Exact confidence interval can be substantially different from the CLT-based interval
  - CLT interval is symmetric about $\bar X_n$, while the exact interval is not
- As $n$ increases
  - CLT-based interval approaches the exact interval

:::

::: {.column width="55%"}

```{python}
#| echo: false
#| fig-width: 6.8
#| fig-height: 3.4

import numpy as np
import matplotlib.pyplot as plt
import scipy.stats as st
plt.rcParams["mathtext.fontset"] = "cm"

n = 16
df = 2*n

mu_hat = xbar
sigma_hat = mu_hat
se_hat = sigma_hat/np.sqrt(n)

# exact pdf of Xbar from chi-square transform:
# if Y ~ chi2_{2n} and Xbar = (mu/(2n)) Y, then
# f_Xbar(x) = f_Y( (2n/mu) x ) * (2n/mu)
def pdf_xbar_exact(x):
    y = (2*n/mu_hat)*x
    return st.chi2.pdf(y, df) * (2*n/mu_hat)

def pdf_xbar_clt(x):
    return st.norm.pdf(x, loc=mu_hat, scale=se_hat)

x_lo = max(0.0, mu_hat - 4*se_hat)
x_hi = mu_hat + 4*se_hat
x = np.linspace(x_lo, x_hi, 900)
# Central 95% regions
a2 = alpha/2

# Exact quantiles via chi-square inversion
q_lo = st.chi2.ppf(a2, df)
q_hi = st.chi2.ppf(1 - a2, df)
x_ex_lo = (mu_hat/(2*n)) * q_lo
x_ex_hi = (mu_hat/(2*n)) * q_hi

# CLT quantiles
x_clt_lo = mu_hat - st.norm.ppf(1 - a2)*se_hat
x_clt_hi = mu_hat + st.norm.ppf(1 - a2)*se_hat

fig, ax = plt.subplots()

# Exact (blue) and CLT normal (orange)
ax.plot(x, pdf_xbar_exact(x), linewidth=4, color="tab:blue")
ax.plot(x, pdf_xbar_clt(x),   linewidth=4, color="tab:orange")

# Shade exact 95% region (blue)
x_mid_ex = x[(x >= x_ex_lo) & (x <= x_ex_hi)]
ax.fill_between(x_mid_ex, 0, pdf_xbar_exact(x_mid_ex),
                color="tab:blue", alpha=0.25)

# Shade CLT 95% region (orange)
x_mid_clt = x[(x >= x_clt_lo) & (x <= x_clt_hi)]
ax.fill_between(x_mid_clt, 0, pdf_xbar_clt(x_mid_clt),
                color="tab:orange", alpha=0.25)

# Exact CI endpoints (blue)
ax.vlines([x_ex_lo, x_ex_hi],
          0,
          [pdf_xbar_exact(x_ex_lo), pdf_xbar_exact(x_ex_hi)],
          linewidth=3,
          color="tab:blue")

# CLT CI endpoints (orange)
ax.vlines([x_clt_lo, x_clt_hi],
          0,
          [pdf_xbar_clt(x_clt_lo), pdf_xbar_clt(x_clt_hi)],
          linewidth=3,
          color="tab:orange")
          
# Label xbar under the axis
ax.annotate(
    rf"$\overline{{X}} = {xbar}$",
    xy=(xbar, 0),
    xycoords=("data", "axes fraction"),
    xytext=(0, -20),
    textcoords="offset points",
    ha="center",
    va="top",
    fontsize=18
)

ax.plot([xbar, xbar],
        [0, -0.05],
        transform=ax.get_xaxis_transform(),
        color="black",
        linewidth=2,
        clip_on=False)

ax.set_title(rf"Exact vs CLT CI for $\mu$ ($n=16$, central {100*(1-alpha):.0f}\%)", fontsize=24)
ax.set_yticks([])
ax.tick_params(axis="x", labelsize=18)

# Simple legend
ax.plot([], [], linewidth=4, color="tab:blue", label="Exact (via $\\chi^2$)")
ax.plot([], [], linewidth=4, color="tab:orange",   label="CLT Normal")
ax.legend(frameon=False, fontsize=16, loc="upper right")

plt.tight_layout()
plt.show()
```

:::

::: 

&nbsp;

<a class="button" style="margin-left:0em;" href="../classlib/classlib/notebooks/coverage_exact_vs_clt.ipynb" download>
â¬‡ Exact vs CLT Confidence Interval Coverage ðŸ““
</a>

---

## Example: CI for a binomial proportion when no failures are observed{#ci-binomial-zero-failures}

You draw $n$ IID samples of your product to test for _failure_, and none of the samples fail.
What is your confidence interval for $p$, the probability that a product is satisfactory?

Let $X_i = 1$ if the $i$th product is satisfactory and $0$ otherwise. Note that
\begin{gather*}
X_i =
\begin{cases}
1, & \text{satisfactory},\\
0, & \text{failure},
\end{cases}
\qquad
X_i \sim \Bern(p), \quad p=\Prob(\text{satisfactory}),
\\
T := \sum_{i=1}^n X_i \quad \text{(\# satisfactory)} \sim \Bin(n,p).
\end{gather*}

- Want a one-sided confidence interval for $p$ of the form $[P_L,1]$; confidence in our product quality

- $P_L$ is *a random variable*, defined as a function of $T$
  - If true success probability $< P_L$, then observing $\ge T$ successes is quite unlikely

- We define a function $p_{L,\alpha} : \{0,1,\ldots,n\} \to [0,1]$ implicitly by requiring that 
$$
\Prob_{\Bin(n,p_{L,\alpha}(t))}\bigl(T \ge t\bigr) = \alpha \qquad \forall t \in \{0,1,\ldots,n\}
$$
The random lower confidence limit is then $P_L := p_{L,\alpha}(T)$

---

$[P_L,1]$ takes the form $P_L := p_{L,\alpha}(T)$, with
$$
\Prob_{\Bin(n,p_{L,\alpha}(t))}\bigl(T \ge t\bigr) = \alpha \qquad \forall t \in \{0,1,\ldots,n\}.
$$
In our case the realized confidence interval based on $n$ successes is $[p_{L,\alpha}(n),1]$, so 
$$
[p_{L,\alpha}(n)]^n = \Prob_{\Bin(n,p_{L,\alpha}(n))}\bigl(T \ge n\bigr) = \alpha \iff p_{L,\alpha}(n) = \alpha^{1/n}
$$

&nbsp;

```{python}
#| echo: false
#| output: asis

alpha = 0.05
ns = [5, 10,20, 100]
vals = [alpha**(1/n) for n in ns]

print("| $n$ | " + " | ".join(str(n) for n in ns) + " |")
print("|:-:|" + "|".join(["---"]*len(ns)) + "|")
print("| $p_L = \\alpha^{1/n}$ | " + " | ".join(f"{v:.4f}" for v in vals) + " |")

```


## Confidence Intervals for _Means_ of Differences

For paired or matched data (before/after, twins, same subject measured twice)
$$
D_i = X_i - Y_i, \quad i = 1,\dots,n
$$

Inference is about the **mean difference** $\mu_D$ (paired setting)

_Not_ difference of means $\mu_X - \mu_Y$ (unpaired), even though $\barD_n= \barX_n - \barY_n$

If $D_1,\dots,D_n \IIDsim \Norm(\mu_D, \sigma_D^2)$
$$
\Prob\left[ \barD_n - t_{n-1,\alpha/2}\frac{S_{D,n}}{\sqrt{n}} \le \mu_D \le \barD_n  + t_{n-1,\alpha/2}\frac{S_{D,n}}{\sqrt{n}} \right] = 1 - \alpha
$$
where $\displaystyle S_{D,n}^2 = \frac 1{n-1} \sum_{i=1}^n (D_i - \barD_n)^2$

If $D_1,\dots,D_n$ are IID with finite variance, and $n$ is large
$$
\Prob\left[ \barD_n - z_{\alpha/2}\frac{S_{D,n}}{\sqrt{n}} \le \mu_D \le \barD_n + z_{\alpha/2}\frac{S_{D,n}}{\sqrt{n}} \right] \approx 1 - \alpha
$$


## Confidence Intervals for _Differences_ of Means

For two _independent_ samples (control/treatment, two groups)
 
$$
X_1,\dots,X_{n_X} \sim \text{population 1}, \quad
Y_1,\dots,Y_{n_Y} \sim \text{population 2}
$$
with sample means $\barX,\barY$ and sample variances $s_X^2,s_Y^2$

If the $X$ and $Y$ are [Normal]{.alert}, we have an [approximate]{.alert} confidence interval for $\mu_X - \mu_Y$ 

$$
\Prob\left[
(\barX-\barY)
 - 
t_{1-\alpha/2,\nu}
\sqrt{\frac{S_X^2}{n_X}+\frac{S_Y^2}{n_Y}} \le \mu_X - \mu_Y \le
(\barX-\barY)
 + 
t_{1-\alpha/2,\nu}
\sqrt{\frac{S_X^2}{n_X}+\frac{S_Y^2}{n_Y}}  
\right] = 1 - \alpha
$$

where $\nu$ is an approximate degrees of freedom (value not critical here)


Other variations exist, e.g., pooled *t*, Welch

--- 

If $n_X, n_Y$ are large, we have a **CLT-based interval**

$$
\Prob\left[
(\barX-\barY)
 - z_{1-\alpha/2}
\sqrt{\frac{S_X^2}{n_X}+\frac{S_Y^2}{n_Y}} \le \mu_X - \mu_Y \le
(\barX-\barY)
 + z_{1-\alpha/2}
\sqrt{\frac{S_X^2}{n_X}+\frac{S_Y^2}{n_Y}}  
\right] \approx 1 - \alpha
$$


## Confidence Intervals for _Proportions_

**Typical setting:** Bernoulli trials

Let  
\[
X \sim \text{Binomial}(n,p), \quad \hat p = X/n
\]

Parameter of interest: population proportion $p$.

**Common methods**

- **Normal approximation interval**
  - Requires $np$ and $n(1-p)$ large
  - Simple but can undercover

- **Wilson / Agrestiâ€“Coull interval**
  - Improved accuracy over normal approximation
  - Near-nominal coverage for moderate $n$

- **Exact (Clopperâ€“Pearson) interval**
  - No sample size restriction
  - Guaranteed coverage
  - Conservative (wider than necessary)

**Key tradeoff:** simplicity vs coverage accuracy.

## Confidence Intervals for _Variances_

**Typical setting:** inference about variability

Parameter of interest: population variance $\sigma^2$.

**Common methods**

- **Chi-squared interval**
  - Assumes data are IID Normal
  - Exact for any $n$
  - Very sensitive to non-normality

- **Asymptotic (CLT-based) intervals**
  - Assume finite fourth moments
  - Require large $n$
  - Rarely used in introductory courses

**Important:** variance inference is far less robust than mean inference.

## Assumptions Behind Common Confidence Intervals

Data are IID from a distribution with finite variance

| Parameter | Distributional Assumptions | Sample Size | Method | Notes |
|-------|--------------------------------|--------|-----------------------|----------------------------|
| $\mu$ | **Any** distribution | **Large** $n$ | Central Limit Theorem (CLT) | **Approximate**, accuracy improves as $n \to \infty$ |
| $\mu$ |  **Normal** data, <br>$\sigma$ unknown | Any $n$ | Studentâ€™s *t* | **Exact**|
| $\mu = p$ | **Bernoulli** trials | Any $n$ | Binomial<br>(Clopperâ€“Pearson) | **Exact**<br>Conservative |
| $\mu = p$ | **Bernoulli** trials | Large $np$,  $n(1-p)$ | Central Limit Theorem (CLT) | **Approximate** |
| $\mu$ | **Exponential** data | Any $n$ | Gamma/<br>Chi-squared  | **Exact** |

---

| Parameter | Distributional Assumptions | Sample Size | Method | Notes |
|-------|--------------------------------|--------|-----------------------|----------------------------|
| $\mu_D$ <br>(paired differences)| Differences are **Normal** | Any $n$ | Paired *t* |  **Exact**, sometimes confused with two-sample *t* |
| $\mu_1-\mu_2$ | Each sample **Normal**; independent samples | Any $n_1,n_2$ | Two-sample *t* (Wackerly) | **Approximate**, uses unpooled standard error |
| $\mu_1-\mu_2$  | Independent samples from **any** distributions with finite variances | **Large** $n_1,n_2$ | CLT (two-sample) | **Approximate**|
| 
| $\sigma^2$ | **Normal** data | Any $n$ | Chi-squared | **Exact**, sensitive to non-normality |
| $\med(X)$ | Continuous distribution | $n$ not too small | Order-statistics |**Approximate**, Distribution-free |  

---

### Summary

- Confidence intervals provide a range of plausible values for parameters based on data
- The validity of confidence intervals depends on assumptions about the [data distribution]{.alert} and [sample size(s)]{.alert}
  - With fewer data we need stronger distributional assumptions
  - With more data we can rely on asymptotic results like the CLT