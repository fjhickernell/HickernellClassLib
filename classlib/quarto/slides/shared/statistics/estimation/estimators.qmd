# Estimators/Estimates

[Estimator:]{.alert} a random variable (or function of the sample) used to approximate an unknown parameter
[Estimate:]{.alert} the realized numerical value of an estimator after observing the data

- [Summary statistics](#summary-statistics)
- [Maximum likelihood estimators (MLE)](#maximum-likelihood-estimators)
- [Plug-in estimators](#plug-in-estimators)


## Summary statistics{#summary-statistics}

Given IID data, $X_1, \ldots, X_n$, we often compute

- [Empirical Distribution]{.alert} $F_{\{X_i\}}(x) := \frac 1n \sum_{i=1}^n \indic(X_i \ge x)$

- [Sample Mean]{.alert} $\displaystyle \barX := \frac{1}{n} \sum_{i=1}^n X_i = \Ex_{F_{\{X_i\}}}(X)$ to approximate the population mean $\mu := \Ex[X_1]$

- [Sample Variance]{.alert} $S^2 := \displaystyle \frac{1}{n-1} \sum_{i=1}^n (X_i - \barX)^2$ to approximate the population variance $\sigma^2 := \var(X_1) := \Ex[(X_1-\mu)^2]$
  - Sometimes $\hsigma^2 := \displaystyle \frac{1}{\class{alert}{n}} \sum_{i=1}^n (X_i - \barX)^2$ 

- [Order Statistics]{.alert} $X_{(1)}, X_{(2)}, \ldots$, reorder the data so that 
$$ X_{(1)} \le X_{(2)} \le \cdots \le X_{(n)}, \qquad \text{i.e., } X_{(i)} = Q_{\{X_i\}}(i/n)
$$
where $Q_{\{X_i\}}$ is the quantile function corresponding to the empirical distribution

---

And given IID data, $(X_1, Y_1), \ldots, (X_n,Y_n)$, we often compute

- [Sample Covariance]{.alert} $\displaystyle  S_{XY} := \frac{1}{n-1} \sum_{i=1}^n (X_i - \barX)(Y_i - \barY)$ to approximate the population covariance $\cov(X_1,Y_1) : = \Ex[(X_1 - \mu)(Y_1 - \mu)]$

- [Sample Correlation]{.alert} $\displaystyle  R_{XY} := \frac{S_{XY}}{S_X S_Y}$ to approximate the population correlation $\displaystyle \corr(X_1,Y_1) : = \frac{\cov(X_1,Y_1)}{\sigma_X \sigma_Y}$

## Maximum likelihood estimators{#maximum-likelihood-estimators}

The joint density of data,  $\vX = (X_1, \ldots, X_n)^\top$ given a parameter, $\vtheta$, is $\varrho_{X_1, \ldots, X_n| \vtheta}$.  The [likelihood]{.alert}, $L$ turns that around to make the parameter the variable, so 
\begin{align*}
L(\vtheta | x_1, \ldots, x_n) & := \varrho_{X_1, \ldots, X_n | \vtheta}(x_1, \ldots, x_n) \\
& = \prod_{i=1}^n \varrho_{X_1 |\vtheta}(x_i) \qquad \text{if } X_1, \ldots, X_n \sim \IID
\end{align*}

The [maximum likelihood estimator (MLE)]{.alert} of $\vtheta$ is the one that fits the observed data best in terms of
$$
\vTheta_{\MLE}  = \Argmax{\vtheta} L(\vtheta | X_1, \ldots, X_n) 
$$

$\exstar$ What is the MLE of $p$ for the distribution $\Bern(p)$?

::: {.exitem}
<span class="exbullet">$\exstar$</span><span>What is the MLE of $\lambda$ for $\Exp(\lambda)$? What are the MLE of $\mu=\Ex(X)$ and $\sigma=\var(X)$ for $X\sim\Exp(\lambda)$?</span>
:::

$\exstar$ What are the MLE of $\mu$ and $\sigma$ for $X \sim \Norm(\mu,\sigma^2)$?



## Plug-in estimators{#plug-in-estimators}
- If $\hTheta_1$ is an estimator of $\theta_1$ and $\theta_2 = g(\theta_1)$, then $\hTheta_2 : = g(\htheta_1)$ is a [plug-in estimator]{.alert} of $\theta_2$
- If $\hTheta_1$ is MLE of $\theta_1$ and $\theta_2 = g(\theta_1)$, then $\hTheta_2 : = g(\hTheta_1)$ is an MLE of $\theta_2$

# Properties of Estimators
- [Bias and variance](#biasvar)
- [Distributions of estimators](#distribest)

## Bias, variance, and mean squared error of estimators{#biasvar}
Suppose that $\Theta$ is an estimator of a parameter, $\theta$, of a population

- [Bias]{.alert} $\bias(\Theta) = \Ex(\Theta) - \theta$
  - Asymptotic bias is $\displaystyle \lim_{n \to \infty} \bias(\Theta_n)$, where $n$ is the size of the sample on which the estimator is based
  - An estimator is [unbiased]{.alert} if $\bias(\Theta) = 0$
  - $\barX$ is an unbiased estimator of $\mu = \Ex(X_1)$ for $X_1, \ldots, X_n$ identically distributed

- [Variance]{.alert} we aleady know
  - $\var(\barX) \exeq \var(X_1)/n$ for IID data

- [Mean squared error (MSE)]{.alert} $\mse(\Theta) := \Ex[(\Theta - \theta)^2] \exeq [\bias(\Theta)]^2 + \var(\Theta)$

$\exstar$ Show that $S^2 := \displaystyle \frac{1}{n-1} \sum_{i=1}^n (X_i - \barX)^2$ is an unbiased estimator of $\sigma^2$

$\exstar$ Is the MLE of $\sigma=\var(X)$ for $X\sim\Exp(\lambda)$ unbiased?

$\exstar$ What is the MLE $\theta$ of $\theta$ for $X \sim \Unif(0,\theta)$?  Is it unbiased?  Can you modify it to be unbiased?


## Distributions of estimators

For the sample mean, $\barX_n$

- $n \barX_n \sim \Bin(n,p)$ if $X \sim \Bern(p)$

- $\barX_n \sim \Gam(n, n \lambda)$ if $X \sim \Exp(p)$ where $\varrho_{\Gam(\alpha, \lambda)}(x) = \frac{\lambda^{\alpha}}{\Gamma(\alpha)}\, x^{\alpha-1} \exp(-\lambda x)
\quad x>0$ <br>(Note: $\Gamma(n) = (n-1)!$ for integer $n$)

- $\barX_n \exsim \Norm(\mu,\sigma^2/n)$ if $X \sim \Norm(\mu,\sigma^2)$

- $\barX_n \appxsim \Norm(\mu,\sigma^2/n)$ for arbitrary distributions and large $n$ by the [Central Limit Theorem]{.alert}

- For order statistics, $X_{(k)}$, $F_{X_{(k)}}(x) = \sum_{j=k}^n \binom{n}{j} [F_X(x)]^j [1 - F_X(x)]^{n-j}$ for IID data from CDF $F_X$
